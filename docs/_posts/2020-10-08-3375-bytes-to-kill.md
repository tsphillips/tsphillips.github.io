---
layout: post
title:  "It takes 3,375 bytes to kill over 50 million people."
date:   2020-10-07 07:00:00 EDT
categories: genetics artificial_life malware
---
Today's post is a reprint of a post I made on September 16, 2018, just over a year before SARS-CoV-19 (and the COVID-19 pandemic) engulfed the world. I think it is important that we occassionally reflect of the fragility of the systems around us.

---

Are you concerned about artificial intelligence (AI) or machine learning (ML) getting out of control and wreaking havoc on modern civilization? Or perhaps you worry about simpler things, such as your company being under constant threat of cyber attacks? Take a deep breath -- things are much worse than you imagine. Modern civilization faces an immediate existential threat that is facilitated by our globally interconnected computers. I would argue that this threat is so great that it would make the great depression seem trivial in comparison.

**Many experts are concerned about the dangers of artificial intelligence.**

To many people, the idea of robots taking over the world is a scary thought. Thankfully, it is not so clear that unstoppable killer robots are viable at this time. After all, humans are very good at destroying things, and while a batch of killer robots might do some short term damage, we would probably fair pretty well in an all-out war against our mechanical progeny. The real danger is not with killer robots, but with efficient computers. Recently, a number of technical and scientific luminaries such as Neil Degrasse Tyson and Ray Kurzweil have debated the risks of computers that can exhibit general intelligence [[1]](#1). Intellectual titans such as Steven Hawking [[2]](#2) [[3]](#3), Bill Gates [[3]](#3) [[4]](#4), and Elon Musk [[5]](#5) [[6]](#6) have given somewhat dire warnings that we are headed toward something bad. Professor Stuart Russell, an AI researcher at the University of California in Berkeley and the Centre for the study of Existential Risk at Cambridge University, describes general artificial intelligence as a threat equal to nuclear weapons [[7]](#7).

In February 2018, a team of 26 experts from 14 institutions published a 100-page report on the potential malicious use of artificial intelligence [[8]](#8). Some of the threats discussed in the report included automated phishing, faster hacking, fooling AI systems, automated terrorism, robot swarms, remote physical attacks, political propaganda, automatic dissent removal, and personalized persuasion [[9]](#9). One can make a strong argument that at least some of these threats already have been realized. Clearly, our rapidly increasing computing power has brought with it certain risks.

**Computer programs have demonstrated the ability to cause massive harm.**

After hearing so many warnings that artificial intelligence will make the sky fall, it is easy to grow numb. After all, is there any real, immediate threat to our lives, or are the so-called experts just crying wolf?

Unfortunately, we have definite evidence that computer programs can cause massive harm. This can be financial harm caused by the disruption of business operations, physical harm caused by the disruption of medical services, or even the destruction of critical infrastructure such as nuclear facilities. The hack at Sony Pictures was a significant wake-up call for businesses. Not only did an adversary break in remotely and steal practically all business data, but all information technology equipment was left in an unusable state, forcing the business operations to regress to the pre-computer era [[10]](#10) [[11]](#11) [[12]](#12). The incident at Sony Pictures was proof positive that an adversary can remotely shut down a company.

In 2016, hospitals saw a surge of ransomware attacks -- essentially technological extortion [[13]](#13) [[14]](#14). Remote adversaries will break into hospital computers and encrypt important data. The hospital are left helpless to fulfill its mission until the staff pay a ransom to have the computer equipment unlocked. As if to demonstrate that extortionists have absolutely no sense of morality, more recent attacks include malware that sabotages critical medial equipment such as heart monitors, insulin pumps, and radiology devices such as CT and MRI machines [[15]](#15) [[16]](#16). Yes, if a hospital does not give in to adversary demands, the adversary can kill patients.

Attacks on hospitals look like petty crime when compared to nuclear infrastructure attacks. Stuxnet, the cyber attack against Iran's centrifuges that was revealed circa 2010, showed the world that an appropriately resourced adversary could compromise and destroy even well-hidden and extensively protected nuclear facilities [[17]](#17) [[18]](#18). Yes, it does seem possible that someone could cause catastrophic physical damage to a nuclear power plant.

I would hope that today, no one would disagree that computer systems all around the world are not only vulnerable to attack, but can be used to harm people. However, a collection of cyber crimes seems like a long way from 50 million people sent to an early grave. Where is this going?

**A small amount of information can cause massive harm.**

Most people go through life thinking that tomorrow will be very much like today, except that sometimes it isn't. Nassim Nicholas Taleb, a scholar whose PhD dissertation was on the mathematics of derivatives pricing, wrote a book in 2007 called The Black Swan: The Impact of the Highly Improbable [[19]](#19). (If you have not read this book, I highly recommend it.) The essence of the book was that there are these highly improbable scenarios called Black Swan Events. A Black Swan Event is an outlier event with significant impact that no one anticipated in risk modeling [[20]](#20). An example would be a massive stock market crash that no one anticipated.

It turns out that Black Swan Theory is closely related to a field of mathematics called catastrophe theory. Catastrophe theory is about studying certain mathematical equations that demonstrate sudden and intense qualitative changes in their behavior [[21]](#21). Some names for these mathematical catastrophes include fold, cusp, swallowtail, hyperbolic umbilic, elliptic umbilic, butterfly, and parabolic umbilic, all of which describe the qualitative change in an equation's behavior [[22]](#22). To put all this in layperson's terms, catastrophe theory is the math about under what circumstances tomorrow will look nothing at all like today. And this is the part where we bring up 50 million fatalities.

One hundred years ago, in 1918, World War I was wrapping up. The war took a terrible toll and, at the time, was said to be the "war to end all wars." As the war was ending, the H1N1 influenza virus was mutating into a deadly efficient killer. What soon came to be called the Spanish Flu, the microscopic merciless killer deftly swept across the globe, killing over 50 million people, or an estimated 5% of the world's population [[23]](#23). Today we know much more about that event and we also are able to completely sequence the entire genetic code of the influenza virus [[24]](#24) [[25]](#25). The virus is made of a strand of RNA nucleotides. The letters, G, U, A, and C are used to identify the four types of nucleotides in the virus. Surprisingly, the influenza virus is not nearly as complex as we may expect. The typical influenza virus consists of a strand of approximately 13,500 nucleotides [[26]](#26). At this point we use some simple math to explain this article's title. Because there are only four types of nucleotides, a single nucleotide can be represented by two bits of information. (A bit is a 1 or a 0.) Therefore, an influenza virus contains about 27,000 bits of information (13,500 x 2), which is equivalent to 3,375 bytes. To put that in perspective, this article contains around 6,500 bytes of information. Since this is not a virology article, I have skipped over some details about folding, possible mutations, and other things not necessarily germane to the central argument. The most important takeaway here is that a mere 3,375 bytes of information can have devastating effects on a population of very loosely connected human beings.

Now, imagine what 3,375 bytes of improbably constructed computer code could do to an entire planet of interconnected computers, where transmission occurs just under the speed of light.

**Destructive, self-replicating computer programs can be very small.**

At this point I would expect some readers to claim that a small computer program could never do such great harm to all the computers on the planet. After all, any malicious super-intelligent killer AI would have to be enormously huge and complex, wouldn't it? And isn't that something we would see coming?

I am fairly confident that a global network destroying computer program could, in fact, be approximately 3,375 bytes long. First, there are an enormous number of variations for 3,375 bytes -- 256^3375, in fact. (That number is over 8000 digits.) Second, we already have many examples of small, destructive computer programs seen in the wild. For an extreme example, the "Mini" virus was only 14 bytes [[27]](#27). The "Tinba" virus weighs in at approximately 20,000 bytes and broke into banks circa 2012 [[28]](#28). Regarding destructive power, a 14-byte "fork bomb" program can take down most computers [[29]](#29).

It is both possible and feasible that a small, destructive, self-replicating computer virus could be generated using automated techniques today. The implications of such a creation could be devastating, a true Black Swan Event for the history books. This is not some kind of "with only five more years of research" scenario. This is something that could happen today and would result in tomorrow looking very, very different.

**Mitigating the Risk**

I would be remiss in describing such a significant risk without offering a suggestion for mitigation. Some people think the solution lies in the field of law. If only we pass new laws and issue the right regulations, then we can protect ourselves from an immensely destabilized world. I think that is misguided because no law will prevent the event we wish to avoid. Others think we need to spend more on studying the problem. That certainly has merit, but the risk is immediate. Those with a more militant outlook think a hawkish preemptive position is best. Unfortunately, new weapons fan the flames of a cyber arms race that is already underway. If our goal is to stay safe and avoid catastrophic world-changing events, then my recommendation is that we need to be more mindful of infrastructure. Our current infrastructure needs good stewardship, to include robust architecture, preventative maintenance, and modernization when due. In terms of upgrades, we need intelligent systems, and we need that intelligence focused on risk assessment and self-repair. I have seen too many infrastructures, both physical and computer/network, in absolute shambles. "Just spend what you have to" seems to be the mantra.

By neglecting infrastructure, we have set ourselves up for a 3,375 byte surprise tomorrow morning. Please don't be one of those people who think the highly improbable will not happen. Eventually, anything that can happen, will happen.

**References**

<a name="1">[1]</a> ["Is Artificial Intelligence Dangerous?"](https://www.forbes.com/sites/robertadams/2016/03/25/is-artificial-intelligence-dangerous/#5b27b81358b9)

<a name="2">[2]</a> ["Stephen Hawking: AI could be a 'real danger'"](https://www.cnet.com/news/stephen-hawking-artificial-intelligence-could-be-a-real-danger/)

<a name="3">[3]</a> ["Why Stephen Hawking and Bill Gates Are Terrified of Artificial Intelligence"](https://www.huffingtonpost.com/james-barrat/hawking-gates-artificial-intelligence_b_7008706.html)

<a name="4">[4]</a> ["Bill Gates on dangers of artificial intelligence: ‘I don’t understand why some people are not concerned’"](https://www.washingtonpost.com/news/the-switch/wp/2015/01/28/bill-gates-on-dangers-of-artificial-intelligence-dont-understand-why-some-people-are-not-concerned/?utm_term=.31e2de31ad8e)

<a name="5">[5]</a> ["Elon Musk says we need to regulate AI before it becomes a danger to humanity"](https://www.theverge.com/2017/7/17/15980954/elon-musk-ai-regulation-existential-threat)

<a name="6">[6]</a> ["Elon Musk: ‘Mark my words — A.I. is far more dangerous than nukes’"](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html)

<a name="7">[7]</a> ["'Artificial Intelligence is as dangerous as NUCLEAR WEAPONS': AI pioneer warns smart computers could doom mankind"](https://www.dailymail.co.uk/sciencetech/article-3165356/Artificial-Intelligence-dangerous-NUCLEAR-WEAPONS-AI-pioneer-warns-smart-computers-doom-mankind.html)

<a name="8">[8]</a> ["The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation"](https://maliciousaireport.com/)

<a name="9">[9]</a> ["AI experts list the real dangers of artificial intelligence"](https://qz.com/1213524/ai-experts-list-the-real-dangers-of-artificial-intelligence/)

<a name="10">[10]</a> ["The Sony Pictures hack, explained"](https://www.washingtonpost.com/news/the-switch/wp/2014/12/18/the-sony-pictures-hack-explained/?utm_term=.4d1a4153970e)

<a name="11">[11]</a> ["Timeline: North Korea and the Sony Pictures hack"](https://www.usatoday.com/story/news/nation-now/2014/12/18/sony-hack-timeline-interview-north-korea/20601645/)

<a name="12">[12]</a> ["Sony Pictures hack"](https://en.wikipedia.org/wiki/Sony_Pictures_hack)

<a name="13">[13]</a> ["U.S. hospitals are getting hit by hackers"](https://money.cnn.com/2016/03/23/technology/hospital-ransomware/index.html)

<a name="14">[14]</a> ["Ransomware: See the 14 hospitals attacked so far in 2016"](https://www.healthcareitnews.com/slideshow/ransomware-see-hospitals-hit-2016?page=1)

<a name="15">[15]</a> ["Medical devices are the next security nightmare"](https://www.wired.com/2017/03/medical-devices-next-security-nightmare/)

<a name="16">[16]</a> ["Medical Device Malware Medjack.3 Poses Threat to Hospitals"](https://www.dataprivacyandsecurityinsider.com/2017/03/medical-device-malware-medjack-3-poses-threat-to-hospitals/)

<a name="17">[17]</a> ["To Kill a Centrifuge: A Technical Analysis of What Stuxnet’s Creators Tried to Achieve"](https://www.langner.com/wp-content/uploads/2017/03/to-kill-a-centrifuge.pdf)

<a name="18">[18]</a> ["W32.Stuxnet Dossier"](https://www.symantec.com/content/en/us/enterprise/media/security_response/whitepapers/w32_stuxnet_dossier.pdf)

<a name="19">[19]</a> ["The Black Swan: The Impact of the Highly Improbable"](https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable)

<a name="20">[20]</a> ["Black Swan Events (And Why You Shouldn't Worry About Them)"](https://seekingalpha.com/article/4114741-black-swan-events-worry)

<a name="21">[21]</a> ["Catastrophe theory"](https://en.wikipedia.org/wiki/Catastrophe_theory)

<a name="22">[22]</a> ["Catastrophe"](http://mathworld.wolfram.com/Catastrophe.html)

<a name="23">[23]</a> ["1918 Spanish Flu Pandemic: Spanish influenza killed 5% of the world's population"](https://www.thoughtco.com/1918-spanish-flu-pandemic-1779224)

<a name="24">[24]</a> ["Reconstruction of the 1918 Influenza Pandemic Virus"](https://www.cdc.gov/flu/about/qa/1918flupandemic.htm)

<a name="25">[25]</a> ["Complete genome direct RNA sequencing of influenza A virus"](https://www.biorxiv.org/content/biorxiv/early/2018/04/12/300384.full.pdf)

<a name="26">[26]</a> ["Influenza Virus Genome Sequencing and Genetic Characterization"](https://www.cdc.gov/flu/professionals/laboratory/genetic-characterization.htm)

<a name="27">[27]</a> ["Mini"](http://virus.wikidot.com/mini)

<a name="28">[28]</a> ["World's smallest banking Trojan discovered"](http://www.abc.net.au/pm/content/2012/s3517912.htm)

<a name="29">[29]</a> ["Fork bomb"](https://en.wikipedia.org/wiki/Fork_bomb)
